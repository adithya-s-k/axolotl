{
    "base_model": "Base model: Selects the base model type",
    "base_model_ignore_patterns": "You can specify an ignore pattern if the model repo contains more than 1 model type (*.pt, etc)",
    "base_model_config": "If the base_model repo on hf hub doesn't include configuration .json files,You can set that here, or leave this empty to default to base_model",
    "revision_of_model": "Optional: Specify a model revision from huggingface hub",
    "tokenizer_config": "Optional: Path to tokenizer configuration file",
    "model_type": "Specify the type of model to load (e.g., AutoModelForCausalLM)",
    "trust_remote_code": "Trust remote code for untrusted source",
    "is_falcon_derived_model": "(Internal use only) Identify if the model is based on Falcon",
    "is_llama_derived_model": "(Internal use only) Identify if the model is based on Llama",
    "is_qwen_derived_model": "(Internal use only) Identify if the model is based on Qwen",
    "is_mistral_derived_model": "(Internal use only) Identify if the model is based on Mistral",
    "overrides_of_model_config": {
        "rope_scaling": {
            "type": "RoPE Scaling type (linear | dynamic)",
            "factor": "RoPE Scaling factor"
        }
    },
    "tokenizer_type": "Specify the tokenizer type (e.g., AutoTokenizer)",
    "tokenizer_use_fast": "Use fast option for tokenizer loading from_pretrained",
    "tokenizer_legacy": "Whether to use the legacy tokenizer setting",
    "resize_token_embeddings_to_32x": "Resize the model embeddings when new tokens are added to multiples of 32",
    "bnb_config_kwargs": {
        "llm_int8_has_fp16_weight": "Whether LLM int8 has fp16 weight",
        "bnb_4bit_quant_type": "BitsAndBytes 4-bit quantization type",
        "bnb_4bit_use_double_quant": "Whether to use double quantization for 4-bit"
    },
    "gptq": "Whether you are training a 4-bit GPTQ quantized model",
    "load_in_8bit": "Load the model in 8-bit format",
    "load_in_4bit": "Use BitsAndBytes 4-bit",
    "bf16": "Use CUDA bf16",
    "fp16": "Use CUDA fp16",
    "tf32": "Use CUDA tf32",
    "bfloat16": "Use CUDA bfloat16",
    "float16": "Use CUDA float16",
    "gpu_memory_limit": "Limit the memory for all available GPUs",
    "lora_on_cpu": "Do the LoRA/PEFT loading on CPU",
    "datasets": [
        {
            "path": "Path to dataset",
            "type": "Type of prompt to use for training",
            "ds_type": "Optional: Dataset type (json | arrow | parquet | text | csv)",
            "data_files": "Optional: Path to source data files",
            "shards": "Optional: Number of shards to split data into",
            "name": "Optional: Name of dataset configuration to load",
            "train_on_split": "Optional: Name of dataset split to load from",
            "conversation": "Optional: Fastchat conversation type (only used with type: sharegpt)",
            "field_human": "Optional: Human key to use for conversation",
            "field_model": "Optional: Assistant key to use for conversation",
            "roles": {
                "input": "Optional: List of input roles",
                "output": "Optional: List of output roles"
            },
            "format": "Optional: Format of the custom user instruction prompt",
            "no_input_format": "Optional: Format of the custom user instruction prompt without input",
            "field": "Optional: Field to use for completion datasets"
        }
    ],
    "test_datasets": [
        {
            "path": "Path to evaluation dataset",
            "ds_type": "Dataset type",
            "split": "Dataset split",
            "type": "Dataset type (completion, etc)",
            "data_files": "Path to data files"
        }
    ],
    "rl": "Use RL training",
    "chat_template": "Save the desired chat template to the tokenizer_config.json for easier inferencing",
    "default_system_message": "Change the default system message",
    "axolotl": "Attempt to save the dataset as an arrow after packing the data together",
    "push_dataset_to_hub": "Push prepared dataset to Hub",
    "hub_model_id": "Push checkpoints to Hub",
    "hub_strategy": "Specify how to push checkpoints to Hub",
    "hf_use_auth_token": "Use hf use_auth_token for loading datasets",
    "val_set_size": "Set aside a portion of the dataset for evaluation",
    "sequence_len": "Maximum length of an input to train with",
    "pad_to_sequence_len": "Pad inputs to a constant sequence length",
    "sample_packing": "Use efficient multi-packing",
    "eval_sample_packing": "Sample packing optimization during evaluation",
    "lora": "Specify whether to use LoRA or not",
    "lora_model_dir": "Path to a pre-trained LoRA model directory",
    "lora_r": "LoRA hyperparameter r",
    "lora_alpha": "LoRA hyperparameter alpha",
    "lora_dropout": "LoRA hyperparameter dropout",
    "lora_target_modules": "Target modules for LoRA transformation",
    "peft_layers_to_transform": "Layer indices to transform for PEFT",
    "lora_modules_to_save": "Modules to save for LoRA",
    "lora_fan_in_fan_out": "LoRA hyperparameter fan-in and fan-out",
    "peft": "PEFT configuration",
    "loftq_config": "LoFTQ initialization configuration",
    "tokens": "Add extra tokens",
    "early_stopping_patience": "Stop training after this many evaluation losses have increased in a row",
    "optimizer": "Specify optimizer",
    "optim_args": "Dictionary of arguments to pass to the optimizer",
    "optim_target_modules": "Specify the target modules to optimize",
    "weight_decay": "Specify weight decay",
    "max_grad_norm": "Specify gradient clipping max norm",
    "neftune_noise_alpha": "Augmentation technique for NEFT",
    "flash_optimum": "Use flash optimum",
    "xformers_attention": "Use xformers attention",
    "sdp_attention": "Use scaled-dot-product attention",
    "s2_attention": "Use shifted-sparse attention",
    "resume_from_checkpoint": "Resume training from a specific checkpoint directory",
    "auto_resume_from_checkpoints": "Automatically resume training from checkpoints",
    "output_dir": "Directory to save the full-finetuned model to",
    "wandb_mode": "Wandb mode",
    "wandb_project": "Wandb project name",
    "wandb_entity": "Wandb Team name if using a Team",
    "wandb_watch": "Wandb watch",
    "wandb_name": "Name of the wandb run",
    "wandb_run_id": "ID of the wandb run",
    "wandb_log_model": "Log model to wandb Artifacts",
    "mlflow_tracking_uri": "URI to mlflow",
    "mlflow_experiment_name": "Experiment name for mlflow",
    "hf_mlflow_log_artifacts": "Log artifacts to mlflow",
    "device_map": "Device map for torch distributed training",
    "max_memory": "Maximum memory usage per GPU on the system",
    "local_rank": "Local rank for accelerate and torchrun",
    "fsdp": "FSDP configuration",
    "fsdp_config": "FSDP config arguments",
    "deepspeed": "Deepspeed config path",
    "ddp_timeout": "Advanced DDP argument: Timeout",
    "ddp_bucket_cap_mb": "Advanced DDP argument: Bucket capacity in MB",
    "ddp_broadcast_buffers": "Advanced DDP argument: Broadcast buffers",
    "torchdistx_path": "Path to torch distx for optim 'adamw_anyprecision'",
    "pretraining_dataset": "Path to HF dataset for type: 'completion' for streaming",
    "debug": "Debug mode",
    "seed": "Random seed",
    "strict": "Allow overwrite YAML config using from CLI"
}
